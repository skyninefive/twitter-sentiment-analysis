{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import re\n",
    "import sys\n",
    "from utils import write_status\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    # 删除标点符号(语料是英文就没写中文版的标点符号)\n",
    "    word = word.strip('\\'\"?!,.():;')\n",
    "    # 英文当中有重复某个字母来表达强烈情绪的用法...\n",
    "    # 比如: funnnnny --> funny\n",
    "    # 所以要删除掉这种重复.\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # 删除 - 和 '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def is_valid_word(word):\n",
    "    # 检测word是不是字母开头\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def handle_emojis(tweet):\n",
    "    # 处理一些字符表情...把他们分为 EMO_POS 和 EMO_NEG两种\n",
    "    # 中文一般都不用这种简单的东西..都是升级版的, 比如✧(≖ ◡ ≖✿)嘿嘿\n",
    "    \n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    # 处理tweet\n",
    "    processed_tweet = []\n",
    "    # 变为小写字母\n",
    "    tweet = tweet.lower()\n",
    "    # 把URL替换为 'URL' 这个标记词\n",
    "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
    "    # 把@XXX的文本, 替换为 'USER_MENTION'  这个标记词\n",
    "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
    "    # 把#XXX这种tag的#去掉\n",
    "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "    # 删除转帖标识符\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    # ...这种太多点了, 只留一个\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    # 删掉 空格, \" and ' \n",
    "    tweet = tweet.strip(' \"\\'')\n",
    "    # 替换字符表情符为 EMO_POS or EMO_NEG\n",
    "    tweet = handle_emojis(tweet)\n",
    "    # 很多空格的话, 只留一个\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    words = tweet.split()\n",
    "\n",
    "    for word in words:\n",
    "        word = preprocess_word(word)\n",
    "        if is_valid_word(word):\n",
    "            if use_stemmer:\n",
    "                \n",
    "                # 提取词干, 把某词的 名词, 动词, 动名词, 形容词, 过去时等都用一个词表示\n",
    "                # 例如: excited, exciting, excit 提取词干后都对应 excit\n",
    "                word = str(porter_stemmer.stem(word))\n",
    "            processed_tweet.append(word)\n",
    "\n",
    "    return ' '.join(processed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_csv(csv_file_name, processed_file_name, test_file=False):\n",
    "    '''\n",
    "    test_file: 如果是测试集, 就改为True, 这样处理后不写入label\n",
    "    \n",
    "    实际上用pandas就很好做了, 这个适合 数据多+内存小的情况\n",
    "    '''\n",
    "    save_to_file = open(processed_file_name, 'w')\n",
    "\n",
    "    with open(csv_file_name, 'r') as csv:\n",
    "        lines = csv.readlines()\n",
    "        total = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            tweet_id = line[:line.find(',')]\n",
    "            if not test_file:\n",
    "                line = line[1 + line.find(','):]\n",
    "                positive = int(line[:line.find(',')])\n",
    "            line = line[1 + line.find(','):]\n",
    "            tweet = line\n",
    "            processed_tweet = preprocess_tweet(tweet)\n",
    "            if not test_file:\n",
    "                save_to_file.write('%s,%d,%s\\n' %\n",
    "                                   (tweet_id, positive, processed_tweet))\n",
    "            else:\n",
    "                save_to_file.write('%s,%s\\n' %\n",
    "                                   (tweet_id, processed_tweet))\n",
    "            write_status(i + 1, total)\n",
    "    save_to_file.close()\n",
    "    print '\\nSaved processed tweets to: %s' % processed_file_name\n",
    "    return processed_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# main\n",
    "path = './dataset/'\n",
    "csv_file_name = 'train'\n",
    "processed_file_name = path + csv_file_name + '_processed.csv'\n",
    "\n",
    "use_stemmer = False\n",
    "if use_stemmer:\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    processed_file_name = path + csv_file_name + '_processed_stemmed.csv'\n",
    "preprocess_csv(path + csv_file_name + '.csv', processed_file_name, test_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# stats.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "from nltk import FreqDist\n",
    "import pickle\n",
    "import sys\n",
    "from utils import write_status\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 处理 预处理后的csv文件, 得到统计信息\n",
    "# 得到词频和bigram的词频信息.保存在pickle文件.\n",
    "# Takes in a preprocessed CSV file and gives statistics\n",
    "# Writes the frequency distribution of words and bigrams\n",
    "# to pickle files.\n",
    "\n",
    "\n",
    "def analyze_tweet(tweet):\n",
    "    # 分别统计各种类型的次数\n",
    "    result = {}\n",
    "    result['MENTIONS'] = tweet.count('USER_MENTION')\n",
    "    result['URLS'] = tweet.count('URL')\n",
    "    result['POS_EMOS'] = tweet.count('EMO_POS')\n",
    "    result['NEG_EMOS'] = tweet.count('EMO_NEG')\n",
    "    \n",
    "    # 去掉标识词, 再统计单词数量\n",
    "    tweet = tweet.replace('USER_MENTION', '').replace(\n",
    "        'URL', '')\n",
    "    words = tweet.split()\n",
    "    result['WORDS'] = len(words)\n",
    "    \n",
    "    # 构建bigram, 统计bigram数量.\n",
    "    bigrams = get_bigrams(words)\n",
    "    result['BIGRAMS'] = len(bigrams)\n",
    "    return result, words, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigrams(tweet_words):\n",
    "    # 以tuple形式得到bigram\n",
    "    bigrams = []\n",
    "    num_words = len(tweet_words)\n",
    "    for i in xrange(num_words - 1):\n",
    "        bigrams.append((tweet_words[i], tweet_words[i + 1]))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigram_freqdist(bigrams):\n",
    "    # 统计bigram的频数\n",
    "    freq_dict = {}\n",
    "    for bigram in bigrams:\n",
    "        if freq_dict.get(bigram):\n",
    "            freq_dict[bigram] += 1\n",
    "        else:\n",
    "            freq_dict[bigram] = 1\n",
    "    counter = Counter(freq_dict)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化一堆东西...\n",
    "num_tweets, num_pos_tweets, num_neg_tweets = 0, 0, 0\n",
    "num_mentions, max_mentions = 0, 0\n",
    "num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "num_urls, max_urls = 0, 0\n",
    "num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0\n",
    "num_bigrams, num_unique_bigrams = 0, 0\n",
    "all_words = []\n",
    "all_bigrams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 一条一条读数据, 进行统计\n",
    "preprocessed_file_name = './dataset/train_processed.csv'\n",
    "unique_words_file_name = './dataset/train_processed_unique.txt'\n",
    "pkl_file_name = './dataset/train_processed_freqdist.pkl'\n",
    "bi_pkl_file_name = './dataset/train_processed_freqdist_bi.pkl'\n",
    "\n",
    "# 下面这一段代码觉得写的不太好看...\n",
    "with open(preprocessed_file_name, 'r') as csv:\n",
    "    lines = csv.readlines()\n",
    "    num_tweets = len(lines)\n",
    "    for i, line in enumerate(lines):\n",
    "        t_id, if_pos, tweet = line.strip().split(',')\n",
    "        if_pos = int(if_pos)\n",
    "        if if_pos:\n",
    "            num_pos_tweets += 1\n",
    "        else:\n",
    "            num_neg_tweets += 1\n",
    "        result, words, bigrams = analyze_tweet(tweet)\n",
    "        num_mentions += result['MENTIONS']\n",
    "        max_mentions = max(max_mentions, result['MENTIONS'])\n",
    "        num_pos_emojis += result['POS_EMOS']\n",
    "        num_neg_emojis += result['NEG_EMOS']\n",
    "        max_emojis = max(\n",
    "            max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "        num_urls += result['URLS']\n",
    "        max_urls = max(max_urls, result['URLS'])\n",
    "        num_words += result['WORDS']\n",
    "        min_words = min(min_words, result['WORDS'])\n",
    "        max_words = max(max_words, result['WORDS'])\n",
    "        all_words.extend(words)\n",
    "        num_bigrams += result['BIGRAMS']\n",
    "        all_bigrams.extend(bigrams)\n",
    "        write_status(i + 1, num_tweets)\n",
    "num_emojis = num_pos_emojis + num_neg_emojis\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "\n",
    "with open(unique_words_file_name, 'w') as uwf:\n",
    "    uwf.write('\\n'.join(unique_words))\n",
    "num_unique_words = len(unique_words)\n",
    "num_unique_bigrams = len(set(all_bigrams))\n",
    "print '\\nCalculating frequency distribution'\n",
    "\n",
    "# Unigrams\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "with open(pkl_file_name, 'wb') as pkl_file:\n",
    "    pickle.dump(freq_dist, pkl_file)\n",
    "print 'Saved uni-frequency distribution to %s' % pkl_file_name\n",
    "\n",
    "\n",
    "# Bigrams\n",
    "bigram_freq_dist = get_bigram_freqdist(all_bigrams)\n",
    "\n",
    "with open(bi_pkl_file_name, 'wb') as pkl_file:\n",
    "    pickle.dump(bigram_freq_dist, pkl_file)\n",
    "print 'Saved bi-frequency distribution to %s' % bi_pkl_file_name\n",
    "print '\\n[Analysis Statistics]'\n",
    "print 'Tweets => Total: %d, Positive: %d, Negative: %d' % (num_tweets, num_pos_tweets, num_neg_tweets)\n",
    "print 'User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions)\n",
    "print 'URLs => Total: %d, Avg: %.4f, Max: %d' % (num_urls, num_urls / float(num_tweets), max_urls)\n",
    "print 'Emojis => Total: %d, Positive: %d, Negative: %d, Avg: %.4f, Max: %d' % (num_emojis, num_pos_emojis, num_neg_emojis, num_emojis / float(num_tweets), max_emojis)\n",
    "print 'Words => Total: %d, Unique: %d, Avg: %.4f, Max: %d, Min: %d' % (num_words, num_unique_words, num_words / float(num_tweets), max_words, min_words)\n",
    "print 'Bigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_bigrams, num_unique_bigrams, num_bigrams / float(num_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LSTM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import LSTM\n",
    "import utils\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs classification using LSTM network.\n",
    "\n",
    "FREQ_DIST_FILE = './dataset/train_processed_freqdist.pkl'\n",
    "BI_FREQ_DIST_FILE = './dataset/train_processed_freqdist_bi.pkl'\n",
    "TRAIN_PROCESSED_FILE = './dataset/train_processed.csv'\n",
    "TEST_PROCESSED_FILE = './dataset/test_processed.csv'\n",
    "GLOVE_FILE = './dataset/glove_seeds.txt'\n",
    "dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove_vectors(vocab):\n",
    "    print 'Looking for GLOVE vectors'\n",
    "    glove_vectors = {}\n",
    "    found = 0\n",
    "    with open(GLOVE_FILE, 'r') as glove_file:\n",
    "        for i, line in enumerate(glove_file):\n",
    "            utils.write_status(i + 1, 0)\n",
    "            tokens = line.split()\n",
    "            word = tokens[0]\n",
    "            if vocab.get(word):\n",
    "                vector = [float(e) for e in tokens[1:]]\n",
    "                glove_vectors[word] = np.array(vector)\n",
    "                found += 1\n",
    "    print '\\n'\n",
    "    print 'Found %d words in GLOVE' % found\n",
    "    return glove_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_vector(tweet):\n",
    "    words = tweet.split()\n",
    "    feature_vector = []\n",
    "    for i in range(len(words) - 1):\n",
    "        word = words[i]\n",
    "        if vocab.get(word) is not None:\n",
    "            feature_vector.append(vocab.get(word))\n",
    "    if len(words) >= 1:\n",
    "        if vocab.get(words[-1]) is not None:\n",
    "            feature_vector.append(vocab.get(words[-1]))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_tweets(csv_file, test_file=True):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    print 'Generating feature vectors'\n",
    "    with open(csv_file, 'r') as csv:\n",
    "        lines = csv.readlines()\n",
    "        total = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if test_file:\n",
    "                tweet_id, tweet = line.split(',')\n",
    "            else:\n",
    "                tweet_id, sentiment, tweet = line.split(',')\n",
    "            feature_vector = get_feature_vector(tweet)\n",
    "            if test_file:\n",
    "                tweets.append(feature_vector)\n",
    "            else:\n",
    "                tweets.append(feature_vector)\n",
    "                labels.append(int(sentiment))\n",
    "            utils.write_status(i + 1, total)\n",
    "    print '\\n'\n",
    "    return tweets, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = len(sys.argv) == 1\n",
    "np.random.seed(1337)\n",
    "vocab_size = 90000\n",
    "batch_size = 128\n",
    "max_length = 40\n",
    "filters = 128\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#返回词频最多的vocab_size个词, 每个词对应的value是index\n",
    "vocab = utils.top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
    "\n",
    "# 如果下载了pre-train的glove词典则去查词\n",
    "# glove_vectors = get_glove_vectors(vocab)\n",
    "\n",
    "# 将tweer转化为单词index的形式\n",
    "tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
    "# 初始化Embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01 # mean=0, std=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加入没有下载pre-train的词典, 则令glove_vetors={}\n",
    "glove_vectors = {}\n",
    "\n",
    "# 将glove的词向量替换 Embedding matrix中的row\n",
    "for word, i in vocab.items():\n",
    "    glove_vector = glove_vectors.get(word)\n",
    "    if glove_vector is not None:\n",
    "        embedding_matrix[i] = glove_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置最大句子长度max_length, \n",
    "# 例如30, 如果句子没有30个词, 则补index 0, \n",
    "# 如果超过30, 则去掉后面的词\n",
    "tweets = pad_sequences(tweets, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 打乱索引\n",
    "shuffled_indices = np.random.permutation(tweets.shape[0])\n",
    "\n",
    "tweets = tweets[shuffled_indices]\n",
    "labels = labels[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(filters))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "filepath = \"./models/lstm-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n",
    "\n",
    "# 设置在每个epoch后储存模型, 只保存val_loss最小的model\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# 如果val_loss在patience个epoch后还不变, 就线性减小学习率, new_lr = lr * factor\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
    "print model.summary()\n",
    "\n",
    "model.fit(tweets, labels, batch_size=batch_size, epochs=1, validation_split=0.1, \\\n",
    "          shuffle=True, callbacks=[checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 在测试集测试模型效果. \n",
    "\n",
    "# 选择模型\n",
    "model = load_model('./models/XXXXX')\n",
    "print model.summary()\n",
    "\n",
    "test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
    "test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
    "predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
    "results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
    "utils.save_results_to_csv(results, 'lstm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
